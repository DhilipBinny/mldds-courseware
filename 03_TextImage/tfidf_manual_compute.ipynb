{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "text processing - tfidf manual compute.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lisaong/mldds-courseware/blob/master/03_TextImage/tfidf_manual_compute.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZthx2dwLJeB",
        "colab_type": "text"
      },
      "source": [
        "# Text Processing\n",
        "\n",
        "This notebook covers:\n",
        "- Text Processing Techniques\n",
        "- Count vectorization\n",
        "- TFIDF computation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7CxXBGmLJeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCPLNk0yLJf8",
        "colab_type": "text"
      },
      "source": [
        "## Text processing:\n",
        "- Tokenise: split text into words\n",
        "- Lemmatise: look at word form, considering noun, adjective, etc\n",
        "- Stem: get the word stem (bluntly chop off the end)\n",
        "\n",
        "### Goals:\n",
        "- Identify unique words\n",
        "- Avoid duplicating the same word form (e.g. cat, cats) in order to keep number of features small\n",
        "\n",
        "### Curse of Dimensionality:\n",
        "- 1 word is at least 1 feature (not considering N-grams - sequences of N words)\n",
        "- Reducing number of words will improve scalability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVCzOFiULJgD",
        "colab_type": "text"
      },
      "source": [
        "## NLTK\n",
        "\n",
        "We will exploring using NLTK (the Natural Language Processing Toolkit) to perform text pre-processing.\n",
        "\n",
        "(You should already have NLTK installed if you followed the Workshop Setup Instructions.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_DMhj7XLJgH",
        "colab_type": "code",
        "colab": {},
        "outputId": "439d1814-d3ac-470f-f8ee-c453f4c93554"
      },
      "source": [
        "# Download corpus for text pre-processing. These are not included in NLTK automatically because of file size.\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt') # tokenisation\n",
        "nltk.download('wordnet') # lemmatisation\n",
        "nltk.download('stopwords') # stop words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeAbtsnTLJgi",
        "colab_type": "text"
      },
      "source": [
        "## Tokenise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omQSVXsXLJgl",
        "colab_type": "code",
        "colab": {},
        "outputId": "a0b27b13-1a30-4a62-81e4-e41aa8eac523"
      },
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "text = 'Hello this is a test.'\n",
        "\n",
        "word_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'this', 'is', 'a', 'test', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yd_8OH7VLJgr",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxXHmCMdLJgt",
        "colab_type": "code",
        "colab": {},
        "outputId": "28b2c023-6aca-4587-a6d7-71c1ab7521d5"
      },
      "source": [
        "text = 'he liked cats and dogs, and teaching machines to learn'\n",
        "\n",
        "# just tokenisation\n",
        "word_tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he',\n",
              " 'liked',\n",
              " 'cats',\n",
              " 'and',\n",
              " 'dogs',\n",
              " ',',\n",
              " 'and',\n",
              " 'teaching',\n",
              " 'machines',\n",
              " 'to',\n",
              " 'learn']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQdXXdpILJgy",
        "colab_type": "code",
        "colab": {},
        "outputId": "ae4604ea-33db-4083-ff01-dee683151652"
      },
      "source": [
        "# WordNet is a lexical database, it is used to\n",
        "# find the lemma of a word based on language rules (verb, etc)\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "text = 'he liked cats and dogs, and teaching machines to learn fasting mice women men man'\n",
        "\n",
        "# tokenise: breaks up sentence into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# lemmatise using list comprehension\n",
        "# WordNetLemmatizer\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "# result = [] \n",
        "# for t in tokens:\n",
        "#     print(wnl.lemmatize(t))\n",
        "#     result.append(wnl.lemmatize(t))\n",
        "\n",
        "# creates a list of lemmatised tokens\n",
        "# \n",
        "# text = 'he liked cats and dogs, and teaching machines to learn fasting mice men women'\n",
        "\n",
        "[wnl.lemmatize(t) for t in tokens]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he',\n",
              " 'liked',\n",
              " 'cat',\n",
              " 'and',\n",
              " 'dog',\n",
              " ',',\n",
              " 'and',\n",
              " 'teaching',\n",
              " 'machine',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'fasting',\n",
              " 'mouse',\n",
              " 'woman',\n",
              " 'men',\n",
              " 'man']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnP9CCiXLJg4",
        "colab_type": "text"
      },
      "source": [
        "## Stem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUSj4qIsLJg7",
        "colab_type": "code",
        "colab": {},
        "outputId": "78797dcf-9490-4b7b-bc21-c4c5039b73a3"
      },
      "source": [
        "# Stemmer looks at word endings and chops it off\n",
        "# based on some rules, e.g: es -> e\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "text = 'he liked cats and dogs, and teaching teach machines to learn fasting mice men women'\n",
        "\n",
        "# tokenise\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# stemmer for English\n",
        "stm = SnowballStemmer('english')\n",
        "\n",
        "[stm.stem(t) for t in tokens]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['he',\n",
              " 'like',\n",
              " 'cat',\n",
              " 'and',\n",
              " 'dog',\n",
              " ',',\n",
              " 'and',\n",
              " 'teach',\n",
              " 'teach',\n",
              " 'machin',\n",
              " 'to',\n",
              " 'learn',\n",
              " 'fast',\n",
              " 'mice',\n",
              " 'men',\n",
              " 'women']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF89bMrOLJhC",
        "colab_type": "text"
      },
      "source": [
        "## Stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tETUTvyjLJhD",
        "colab_type": "code",
        "colab": {},
        "outputId": "d8d8f188-9848-483c-a789-be387a6cfdff"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "text = 'he liked cats and dogs, and teaching machines to learn'\n",
        "\n",
        "# lower case\n",
        "lower = text.lower()\n",
        "\n",
        "# tokenize\n",
        "tokens = word_tokenize(lower)\n",
        "\n",
        "# remove stop words\n",
        "[t for t in tokens if (t not in stop)]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['liked', 'cats', 'dogs', ',', 'teaching', 'machines', 'learn']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz2XQdJKLJhJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "e96b71c3-7652-49e7-9ccb-cd40fdfa65c3"
      },
      "source": [
        "stop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okfZ9IjYLJhQ",
        "colab_type": "code",
        "colab": {},
        "outputId": "b4d4c20c-0480-4dca-e24f-fb726e638e9a"
      },
      "source": [
        "len(stop) # number of stop words"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMLkcAaqLJhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add / remove words to stoplist\n",
        "# This changes the local copy, not the original stoplist\n",
        "\n",
        "stop.add('lah')\n",
        "stop.remove('because') # changes the stop list in-place\n",
        "                       # so running this cell again will return\n",
        "                       # KeyError"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHC1dEHhLJhj",
        "colab_type": "text"
      },
      "source": [
        "# Vectorise Text\n",
        "\n",
        "Vectorisation converts words into vectors of numbers\n",
        "\n",
        "Common ways:\n",
        "- By word count: CountVectorizer\n",
        "- By word and document frequency: TfidfVectorizer\n",
        "- By word vectors (gensim Word2Vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkBjXJeFLJhm",
        "colab_type": "text"
      },
      "source": [
        "## Word Count Vectorisation\n",
        "\n",
        "* Words that are used more frequently get a higher count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fa0IKC9LJhp",
        "colab_type": "code",
        "colab": {},
        "outputId": "5165d020-1b0c-4de3-95ba-50c02d717884"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "   'This is the first document.',\n",
        "   'This document is the second document.',\n",
        "   'And this is the third one.',\n",
        "   'Is this the first document?',\n",
        "]\n",
        "\n",
        "# sklearn's stoplist, with unigram & bigram\n",
        "#countvec = CountVectorizer(ngram_range=(1, 2), stop_words='english')\n",
        "\n",
        "# sklearn stoplist is lousy, use nltk's\n",
        "countvec = CountVectorizer(ngram_range=(1, 2), stop_words=list(stop))\n",
        "result = countvec.fit_transform(corpus)\n",
        "print(result) # sparse matrix, (location) non-zero value"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 3)\t1\n",
            "  (0, 0)\t1\n",
            "  (0, 2)\t1\n",
            "  (1, 6)\t1\n",
            "  (1, 1)\t1\n",
            "  (1, 5)\t1\n",
            "  (1, 0)\t2\n",
            "  (2, 8)\t1\n",
            "  (2, 4)\t1\n",
            "  (2, 7)\t1\n",
            "  (3, 3)\t1\n",
            "  (3, 0)\t1\n",
            "  (3, 2)\t1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXQ4aXSkLJhy",
        "colab_type": "code",
        "colab": {},
        "outputId": "41cd2152-1cb5-4522-9e88-4e6e87146057"
      },
      "source": [
        "# convert sparse matrix to dense matrix using todense()\n",
        "result.todense()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[1, 0, 1, 1, 0, 0, 0, 0, 0],\n",
              "        [2, 1, 0, 0, 0, 1, 1, 0, 0],\n",
              "        [0, 0, 0, 0, 1, 0, 0, 1, 1],\n",
              "        [1, 0, 1, 1, 0, 0, 0, 0, 0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLz6UE4VLJh6",
        "colab_type": "code",
        "colab": {},
        "outputId": "733f5b05-d193-43e7-d752-f701b02c65a8"
      },
      "source": [
        "# vocabulary\n",
        "countvec.get_feature_names()\n",
        "\n",
        "# create dataframe with count vectors as data\n",
        "# and vocabulary as headers\n",
        "\n",
        "# add the original text as the 'text column \n",
        "df = pd.DataFrame(result.todense(),\n",
        "             columns=countvec.get_feature_names())\n",
        "df['text'] = corpus\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>document second</th>\n",
              "      <th>first</th>\n",
              "      <th>first document</th>\n",
              "      <th>one</th>\n",
              "      <th>second</th>\n",
              "      <th>second document</th>\n",
              "      <th>third</th>\n",
              "      <th>third one</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>This is the first document.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>This document is the second document.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>And this is the third one.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Is this the first document?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   document  document second  first  first document  one  second  \\\n",
              "0         1                0      1               1    0       0   \n",
              "1         2                1      0               0    0       1   \n",
              "2         0                0      0               0    1       0   \n",
              "3         1                0      1               1    0       0   \n",
              "\n",
              "   second document  third  third one                                   text  \n",
              "0                0      0          0            This is the first document.  \n",
              "1                1      0          0  This document is the second document.  \n",
              "2                0      1          1             And this is the third one.  \n",
              "3                0      0          0            Is this the first document?  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45R4zSNMLJh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A32KmwDkLJiB",
        "colab_type": "text"
      },
      "source": [
        "## Word and Document Frequency (TF-IDF) Vectorisation\n",
        "\n",
        "- TF: Term Frequency: rewards words commonly used in a document\n",
        "- IDF: Inverse Document Frequency: penalises words commonly used in all documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2_zY9ABLJiC",
        "colab_type": "code",
        "colab": {},
        "outputId": "7fd06267-5449-44e4-cec6-d5655094735a"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "   'This document is the first document.',\n",
        "   'This document is the second document.',\n",
        "   'And this is the third one.',\n",
        "   'Is this document the first document?',\n",
        "]\n",
        "\n",
        "# create vectoriser\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 2), stop_words=list(stop))\n",
        "\n",
        "# fit transform\n",
        "result = tfidf.fit_transform(corpus)\n",
        "\n",
        "# inspect matrix\n",
        "result.todense()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0.6829022 , 0.42176004, 0.        , 0.42176004, 0.42176004,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.59329727, 0.        , 0.46475741, 0.        , 0.        ,\n",
              "         0.        , 0.46475741, 0.46475741, 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.57735027, 0.        , 0.        , 0.57735027, 0.57735027],\n",
              "        [0.6829022 , 0.42176004, 0.        , 0.42176004, 0.42176004,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYK6jTWELJiF",
        "colab_type": "code",
        "colab": {},
        "outputId": "5d6569b0-a604-45ad-b6c4-e34e0194017e"
      },
      "source": [
        "# inspect vocabulary using get_feature_names()\n",
        "\n",
        "vocab = tfidf.get_feature_names()\n",
        "vocab"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['document',\n",
              " 'document first',\n",
              " 'document second',\n",
              " 'first',\n",
              " 'first document',\n",
              " 'one',\n",
              " 'second',\n",
              " 'second document',\n",
              " 'third',\n",
              " 'third one']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLoeMtvsLJiJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "d57793f8-65e5-424c-c6d4-b0e96cd61f2e"
      },
      "source": [
        "# create dataframe with count vectors as data\n",
        "# and vocabulary as headers\n",
        "# remember to convert vectors from sparse to dense matrix\n",
        "\n",
        "# add the original text as the 'text column \n",
        "df = pd.DataFrame(result.todense(),\n",
        "             columns=vocab)\n",
        "\n",
        "# add the original text as the 'text column \n",
        "df['text'] = corpus\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>document first</th>\n",
              "      <th>document second</th>\n",
              "      <th>first</th>\n",
              "      <th>first document</th>\n",
              "      <th>one</th>\n",
              "      <th>second</th>\n",
              "      <th>second document</th>\n",
              "      <th>third</th>\n",
              "      <th>third one</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.682902</td>\n",
              "      <td>0.42176</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.42176</td>\n",
              "      <td>0.42176</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>This document is the first document.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.593297</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.464757</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.464757</td>\n",
              "      <td>0.464757</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>This document is the second document.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.57735</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.57735</td>\n",
              "      <td>0.57735</td>\n",
              "      <td>And this is the third one.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.682902</td>\n",
              "      <td>0.42176</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.42176</td>\n",
              "      <td>0.42176</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>Is this document the first document?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   document  document first  document second    first  first document  \\\n",
              "0  0.682902         0.42176         0.000000  0.42176         0.42176   \n",
              "1  0.593297         0.00000         0.464757  0.00000         0.00000   \n",
              "2  0.000000         0.00000         0.000000  0.00000         0.00000   \n",
              "3  0.682902         0.42176         0.000000  0.42176         0.42176   \n",
              "\n",
              "       one    second  second document    third  third one  \\\n",
              "0  0.00000  0.000000         0.000000  0.00000    0.00000   \n",
              "1  0.00000  0.464757         0.464757  0.00000    0.00000   \n",
              "2  0.57735  0.000000         0.000000  0.57735    0.57735   \n",
              "3  0.00000  0.000000         0.000000  0.00000    0.00000   \n",
              "\n",
              "                                    text  \n",
              "0   This document is the first document.  \n",
              "1  This document is the second document.  \n",
              "2             And this is the third one.  \n",
              "3   Is this document the first document?  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RpJJJTOLJiM",
        "colab_type": "code",
        "colab": {},
        "outputId": "d46f3ce4-8643-49bb-d520-0d52b32d898e"
      },
      "source": [
        "# Computing TFIDF manually to check the values above\n",
        "\n",
        "# first, we examine the settings used for our TFIDF vectorizer\n",
        "tfidf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 2), norm='l2', preprocessor=None,\n",
              "                smooth_idf=True,\n",
              "                stop_words=['against', 'lah', 'having', 'mustn', 'above',\n",
              "                            'then', 'hasn', 'as', 'yourself', 'own', 'so',\n",
              "                            'from', 'further', \"you've\", 'between', 'if',\n",
              "                            'yours', 'through', 't', 'only', 'its', 'they',\n",
              "                            \"that'll\", 'been', 'her', 'weren', \"haven't\",\n",
              "                            'over', 'ma', 'am', ...],\n",
              "                strip_accents=None, sublinear_tf=False,\n",
              "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
              "                vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl623qxkLJiQ",
        "colab_type": "text"
      },
      "source": [
        "Based on https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting, the equation used is:\n",
        "\n",
        "$\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}$\n",
        "\n",
        "Where:\n",
        "\n",
        "$\\text{idf}(t) = \\log{\\frac{n}{1+\\text{df}(t)}}$\n",
        "\n",
        "When smooth_idf=True, the TFIDF computed tries to avoid zero values by adding a 1:\n",
        "\n",
        "$\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1$\n",
        "\n",
        "Where $n$ is the total number of documents in the document set, and $\\text{df}(t)$ is the number of documents in the document set that contain term $t$. The resulting tf-idf vectors are then normalized by the Euclidean norm:\n",
        "\n",
        "$v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 +\n",
        "v{_2}^2 + \\dots + v{_n}^2}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jT0suuTvLJiR",
        "colab_type": "code",
        "colab": {},
        "outputId": "80471531-0bec-46c1-e882-e584a598346f"
      },
      "source": [
        "corpus"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This document is the first document.',\n",
              " 'This document is the second document.',\n",
              " 'And this is the third one.',\n",
              " 'Is this document the first document?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOfAtWXBLJiW",
        "colab_type": "code",
        "colab": {},
        "outputId": "9d2f39b0-6d60-429d-c761-266fdea9f74d"
      },
      "source": [
        "# term frequency\n",
        "countvec = CountVectorizer(ngram_range=(1, 2), stop_words=list(stop))\n",
        "\n",
        "tf_sparse = countvec.fit_transform(corpus)\n",
        "tf = tf_sparse.todense()\n",
        "tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[2, 1, 0, 1, 1, 0, 0, 0, 0, 0],\n",
              "        [2, 0, 1, 0, 0, 0, 1, 1, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 1, 0, 0, 1, 1],\n",
              "        [2, 1, 0, 1, 1, 0, 0, 0, 0, 0]], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLBDe_GbLJic",
        "colab_type": "code",
        "colab": {},
        "outputId": "83958552-b3a9-4c4b-b4f0-88141c8ad9b4"
      },
      "source": [
        "# document frequency\n",
        "# the number of documents in the document set that contain term t\n",
        "# can be computed by inspecting how many non-zero counts there are in each column\n",
        "\n",
        "df = (tf > 0).sum(axis=0)\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[3, 2, 1, 2, 2, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihkjdtqKLJio",
        "colab_type": "code",
        "colab": {},
        "outputId": "2b639f0e-0cb0-4468-87d4-2c2b34b16ab0"
      },
      "source": [
        "n = len(corpus)\n",
        "n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEQEIlfTLJiv",
        "colab_type": "code",
        "colab": {},
        "outputId": "7db68724-f964-4382-f561-d2647093777b"
      },
      "source": [
        "# inverse document frequency (smooth_idf=True version)\n",
        "idf = np.log(n / (1 + df)) + 1\n",
        "idf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[1.        , 1.28768207, 1.69314718, 1.28768207, 1.28768207,\n",
              "         1.69314718, 1.69314718, 1.69314718, 1.69314718, 1.69314718]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX9wUlJ9LJi1",
        "colab_type": "code",
        "colab": {},
        "outputId": "fc4b1e0e-23b1-435e-d8a0-46de93d5eae3"
      },
      "source": [
        "# tf x idf\n",
        "# element-wise multiply\n",
        "tfidf_raw = np.multiply(tf, idf)\n",
        "pd.DataFrame(tfidf_raw, columns=vocab)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>document</th>\n",
              "      <th>document first</th>\n",
              "      <th>document second</th>\n",
              "      <th>first</th>\n",
              "      <th>first document</th>\n",
              "      <th>one</th>\n",
              "      <th>second</th>\n",
              "      <th>second document</th>\n",
              "      <th>third</th>\n",
              "      <th>third one</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>1.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.287682</td>\n",
              "      <td>1.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.693147</td>\n",
              "      <td>1.693147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.0</td>\n",
              "      <td>1.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.287682</td>\n",
              "      <td>1.287682</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   document  document first  document second     first  first document  \\\n",
              "0       2.0        1.287682         0.000000  1.287682        1.287682   \n",
              "1       2.0        0.000000         1.693147  0.000000        0.000000   \n",
              "2       0.0        0.000000         0.000000  0.000000        0.000000   \n",
              "3       2.0        1.287682         0.000000  1.287682        1.287682   \n",
              "\n",
              "        one    second  second document     third  third one  \n",
              "0  0.000000  0.000000         0.000000  0.000000   0.000000  \n",
              "1  0.000000  1.693147         1.693147  0.000000   0.000000  \n",
              "2  1.693147  0.000000         0.000000  1.693147   1.693147  \n",
              "3  0.000000  0.000000         0.000000  0.000000   0.000000  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBzHAUpnLJi9",
        "colab_type": "code",
        "colab": {},
        "outputId": "ca5f5a30-929e-470e-acdc-27bc8377a7ce"
      },
      "source": [
        "# compute euclidean norm for each document\n",
        "v_norm = np.linalg.norm(tfidf_raw, axis=1)\n",
        "v_norm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.99572618, 3.54968198, 2.93261694, 2.99572618])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EVLzY6vLJjB",
        "colab_type": "code",
        "colab": {},
        "outputId": "e7e246e0-a9a3-4df1-e580-de3780b59762"
      },
      "source": [
        "# [:, None] allows tfidf_raw to be divided by each vector element\n",
        "# for example, first row will be divided by v_norm[0], second row by v_norm[1], etc\n",
        "tfidf = tfidf_raw / v_norm[:, None]\n",
        "tfidf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0.66761776, 0.42983971, 0.        , 0.42983971, 0.42983971,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.56343076, 0.        , 0.4769856 , 0.        , 0.        ,\n",
              "         0.        , 0.4769856 , 0.4769856 , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.57735027, 0.        , 0.        , 0.57735027, 0.57735027],\n",
              "        [0.66761776, 0.42983971, 0.        , 0.42983971, 0.42983971,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP1A1arhLJjJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "cbdc6f6a-9584-47f8-dcc2-9b3ca0470b60"
      },
      "source": [
        "# compare with TfidfVectorizer's output\n",
        "result.todense()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0.6829022 , 0.42176004, 0.        , 0.42176004, 0.42176004,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.59329727, 0.        , 0.46475741, 0.        , 0.        ,\n",
              "         0.        , 0.46475741, 0.46475741, 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.57735027, 0.        , 0.        , 0.57735027, 0.57735027],\n",
              "        [0.6829022 , 0.42176004, 0.        , 0.42176004, 0.42176004,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzSVhMuFLJjO",
        "colab_type": "code",
        "colab": {},
        "outputId": "cbf88e37-3252-48fc-a33e-2f7a66901605"
      },
      "source": [
        "# the values are within +/-0.02, possibly due to floating point rounding error\n",
        "abs(tfidf - result.todense())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "matrix([[0.01528444, 0.00807967, 0.        , 0.00807967, 0.00807967,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.02986651, 0.        , 0.01222819, 0.        , 0.        ,\n",
              "         0.        , 0.01222819, 0.01222819, 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.01528444, 0.00807967, 0.        , 0.00807967, 0.00807967,\n",
              "         0.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KLTro19LJjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C2orQjTLJjZ",
        "colab_type": "text"
      },
      "source": [
        "## Word2Vec\n",
        "\n",
        "Word2Vec generates word-vectors where vectors close together in vector space have similar meanings based on context, and word-vectors distant to each other have differing meanings.\n",
        "\n",
        "### Gensim\n",
        "\n",
        "Gensim is a Python library to train word vectors, and to load pre-trained word vectors.\n",
        "\n",
        "(You should already have Gensim installed if you followed the Workshop Setup Instructions.)\n",
        "\n",
        "https://radimrehurek.com/gensim/models/word2vec.html\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9qi6NRZLJja",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7wy-w93LJja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "corpus = [\n",
        "    'Dashing through the snow',\n",
        "    'In a one-horse open sleigh',\n",
        "    'Over the fields we go',\n",
        "    'Laughing all the way',\n",
        "    'Bells on bob-tail ring',\n",
        "    'Making spirits bright',\n",
        "    'What fun it is to ride and sing a sleighing song tonight'\n",
        "]\n",
        "\n",
        "# split text into words\n",
        "corpus_tokens = [word_tokenize(doc.lower()) for doc in corpus]\n",
        "\n",
        "vector_size=10 # vector representation (typically about 50-100 for larger vocabs)\n",
        "\n",
        "window_size=3 # how many words to see around it (depending on task)\n",
        "\n",
        "# train\n",
        "word2vec = Word2Vec(corpus_tokens, size=vector_size, \n",
        "                    window=window_size,\n",
        "                    min_count=1, workers=4)\n",
        "\n",
        "# model.save(\"word2vec.model\") # save for use later"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LK-iMlxxLJjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lookup vector using wv\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwfmq4iHLJjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find similar words using most_similar\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGqZVSH2LJjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inspect vocabulary using wv.vocab\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9cnIIG3LJjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get word vectors using vectors\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7BwKEUzLJjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To explore word vectors, you can plot them in 2d space\n",
        "# (Notice how useful PCA is?)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "wv_2d = pca.fit_transform(word2vec.wv.vectors)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# scatter plot\n",
        "ax.scatter(wv_2d[:, 0], wv_2d[:, 1])\n",
        "\n",
        "# annotate each point using the words\n",
        "for i, word in enumerate(word2vec.wv.vocab):\n",
        "    ax.annotate(word, (wv_2d[i, 0], wv_2d[i, 1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPbNE4u9LJj2",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained Word2Vec models\n",
        "\n",
        "Instead of training your own word2vec, you can alternatively load the (large!) pre-trained models\n",
        "\n",
        "https://github.com/RaRe-Technologies/gensim-data\n",
        "\n",
        "These are most commonly used when training Neural Networks, because scikit-learn cannot handle non-scalar features well.  :("
      ]
    }
  ]
}