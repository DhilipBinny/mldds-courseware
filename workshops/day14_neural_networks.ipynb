{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Keras\n",
    "Keras Introduction: https://keras.io/\n",
    "\n",
    "Keras Cheatsheet: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Keras_Cheat_Sheet_Python.pdf\n",
    "\n",
    "Keras FAQ: https://keras.io/getting-started/faq/\n",
    "\n",
    "Keras Sequential API: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about Tensorflow?\n",
    "\n",
    "Tensorflow is available as a \"backend\" for Keras. By default, Keras will use Tensorflow to perform deep learning operations.\n",
    "\n",
    "More about backends here: https://keras.io/backend/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Differences between Keras and Scikit-Learn\n",
    "\n",
    "|Sklearn|Keras|\n",
    "|--|--|\n",
    "|Use for Machine Learning and limited Deep Learning (MLPClassifier, MLPRegressor)|Use only for Deep Learning|\n",
    "|Scope: Linear Regression, Logistic Regression, Support Vector Machines, KMeans, PCA, etc|Scope: Deep Learning layers such as Dense, Convolutional, Recurrent|\n",
    "|Only SGDRegressor, SGDClassifier,  MLP* do gradient descent|Exclusively uses gradient descent and back propagation|\n",
    "|Not designed for long-haul training|Designed for long-haul training, supports saving and resuming training|\n",
    "|Limited support for incremental fit|Always fits incrementally, unless you recompile network|\n",
    "|Does not support GPU|Supports GPU|\n",
    "|Does not support Tensorflow|Supports Tensorflow through a backend|\n",
    "|Provides learning_curve() function for learning curve|Uses [Tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard) for learning curve|\n",
    "|Provides cross_validate() function for cross validation|Cross-validation is built into fit()|\n",
    "|Supports fit with univariate y output only|Supports fit with univariate and multi-variate y output. For classification, y must be one-hot (more in the workshop)|\n",
    "\n",
    "There are other minor differences between how the two libraries work. We'll highlight it along the way.\n",
    "\n",
    "**Caution**: always consult documentation (don't assume Keras works like Scikit-learn, otherwise you waste time debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Machine Learning Workflow\n",
    "\n",
    "1. Problem Definition\n",
    "    - Same as you normally would for any machine learning problem. The key difference with Keras is in the choice of neural networks as the model.\n",
    "\n",
    "2. Data Engineering\n",
    "    - Use pandas as you normally would\n",
    "\n",
    "3. Feature Engineering\n",
    "    - Use sklearn as you normally would\n",
    "\n",
    "4. Model Engineering\n",
    "    \n",
    "    a. Define initial neural net\n",
    "        - Define model architecture, such as the input shapes, output shapes, and neural network layers\n",
    "        - model.compile to pick optimiser, loss function, metrics\n",
    "\n",
    "    b. Setup training callbacks:\n",
    "        - Learning curve using Tensorboard\n",
    "        - Early stopping\n",
    "        - [Optional] Model checkpoints to automatically save weights after every epoch\n",
    "    \n",
    "    c. Train model:\n",
    "        - model.fit(): Unlike sklearn, fit() is cumulative (continues progress if you it call repeatedly)\n",
    "\n",
    "        sklearn:\n",
    "        ```\n",
    "            model = SGDRegressor()\n",
    "            model.fit(X_train, y_train)\n",
    "            model.fit(X_train, y_train) # RESTARTS from scratch\n",
    "        ```\n",
    "\n",
    "         Keras:\n",
    "         ```\n",
    "             model.compile()\n",
    "             model.fit(X_train, y_train) \n",
    "             model.fit(X_train, y_train) # RESUMES training from previously\n",
    "         ```\n",
    "         \n",
    "5. Evaluation metrics\n",
    "    - Keras: model.evaluate() - similar to model.score() in sklearn\n",
    "    - Evaluation metrics in sklearn are more comprehensive. Use them here (e.g. classification_report)\n",
    "\n",
    "6. Deployment\n",
    "    - model.save()\n",
    "    - load_model()\n",
    "    - model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we'll walk through a simple Keras example to understand how to use it:\n",
    "\n",
    "https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.datasets.mnist??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "Keras includes some built-in datasets that are useful for learning and practice.\n",
    "\n",
    "https://keras.io/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape # 60000 rows, 28 x 28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c85283f2e8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0]) # display first image (first row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras requires the targets to be categorical (one-hot)\n",
    "# vectors rather than class (label) vectors\n",
    "# This means that we need to convert the target\n",
    "# before passing it to fit() if doing multi-class classification\n",
    "\n",
    "# convert class vectors to categorical vectors\n",
    "# 5 to [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]\n",
    "num_classes = 10 # 10 digits\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.to_categorical(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "This is an example dataset, so not much feature engineering is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model engineering\n",
    "\n",
    "To run tensorboard for viewing the Learning Curve\n",
    "\n",
    "- Launch another Anaconda Prompt (because tensorboard will run in its own console):\n",
    "\n",
    "```\n",
    "(base) conda activate mldds\n",
    "(mldds) cd folder\\to\\this\\notebook\n",
    "(mldds)tensorboard --logdir=logs --host=0.0.0.0\n",
    "```\n",
    "\n",
    "If this is the first time you are launching Tensorboard, you will not see any sessions until you call model.fit():\n",
    "\n",
    "```\n",
    "tensorboard = TensorBoard(log_dir='./logs/mnist_mlp/%d' % time.time())\n",
    "history = model.fit(X_train, y_train, batch_size=128, epochs=10,\n",
    "                    callbacks=[tensorboard], validation_split=.25)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Flatten from 28x28 to 784 (because we are using MLP)\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "# Scale from 0-255 into 0-1\n",
    "# Scaling is generally recommended for neural networks\n",
    "# for faster convergence\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# input: 784, output: 512 => 784 x 512 weights\n",
    "# (512 neurons)\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# input: 512, output: 512 => 512 x 512 weights\n",
    "# (512 neurons)\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# input: 512, output: 10 => 512 x 10 weights\n",
    "# (10 neurons)\n",
    "# softmax converts a set of outputs to probabilities that add up to 1\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "# Param # is W + bias\n",
    "# Dense: input_shape x output_shape + output_shape\n",
    "#  (where input_shape = previous layer's output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.1339 - val_acc: 0.9842\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 14s 226us/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.1485 - val_acc: 0.9827\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.1657 - val_acc: 0.9820\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.1559 - val_acc: 0.9833\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 13s 215us/step - loss: 0.0050 - acc: 0.9989 - val_loss: 0.1554 - val_acc: 0.9828\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.1696 - val_acc: 0.9814ETA: 0s - loss: 0.0047 - acc: 0.998\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 0.0031 - acc: 0.9993 - val_loss: 0.1420 - val_acc: 0.9834\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 13s 217us/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.1469 - val_acc: 0.9836\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0041 - acc: 0.9992 - val_loss: 0.1687 - val_acc: 0.9828\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 15s 252us/step - loss: 0.0034 - acc: 0.9993 - val_loss: 0.1610 - val_acc: 0.9833\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 15s 247us/step - loss: 0.0056 - acc: 0.9989 - val_loss: 0.1566 - val_acc: 0.9849\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 13s 221us/step - loss: 0.0031 - acc: 0.9992 - val_loss: 0.1575 - val_acc: 0.9846 0.0029 -\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 13s 218us/step - loss: 0.0039 - acc: 0.9991 - val_loss: 0.1553 - val_acc: 0.9848\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 13s 220us/step - loss: 0.0049 - acc: 0.9991 - val_loss: 0.1523 - val_acc: 0.9838\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 13s 223us/step - loss: 0.0017 - acc: 0.9996 - val_loss: 0.1618 - val_acc: 0.9837\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0025 - acc: 0.9996 - val_loss: 0.1672 - val_acc: 0.9832\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 13s 224us/step - loss: 0.0021 - acc: 0.9996 - val_loss: 0.1571 - val_acc: 0.9843\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 14s 228us/step - loss: 0.0058 - acc: 0.9990 - val_loss: 0.1641 - val_acc: 0.9827\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 13s 218us/step - loss: 0.0047 - acc: 0.9991 - val_loss: 0.1776 - val_acc: 0.9826\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.0019 - acc: 0.9995 - val_loss: 0.2052 - val_acc: 0.9805\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "import time\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs/mnist_mlp/%d' % time.time())\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=[tensorboard],\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
